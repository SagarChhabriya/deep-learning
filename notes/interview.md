# Top Interview Questions for Data Science Freshers: Model Evaluation, Feature Engineering, and Ensemble Learning Part - 9

hashtag#Model_Evaluation_Optimization_Questions_(Tricky but Basic) 
1. What is cross-entropy loss, and when is it used in classification tasks? 
2. What does the ROC curve represent, and how should AUC be interpreted? 
3. Why is early stopping helpful during model training? 
4. How do learning curves help diagnose underfitting or overfitting? 
5. What is the bias-variance tradeoff, and how do you handle it? 
6. Why is hyperparameter tuning critical, and what are common techniques? 
7. What is k-fold cross-validation, and why is it better than holdout validation? 
8. Why do models sometimes perform worse after tuning or regularization? 
9. What is the difference between L1 and L2 regularization? 
10. How can a confusion matrix provide more insights than accuracy alone?

hashtag#Feature_Engineering_Preprocessing_Questions 
1. Why is handling missing data important, and what are common strategies? 
2. What happens if categorical variables are not encoded properly? 
3. How does feature scaling influence distance-based models? 
4. What are interaction features, and how do they improve model performance? 
5. Why should we be cautious of data leakage during preprocessing? 
6. What are derived features, and how can they give your model an edge? 
7. How do you detect multicollinearity, and why should it be addressed? 
8. When is dimensionality reduction useful, and what are common techniques? 
9. What is target encoding, and what risks does it bring? 
10. How do outliers affect machine learning models?

hashtag#Ensemble_Learning_Questions 
1. What is the key difference between bagging and boosting? 
2. How does Random Forest reduce overfitting compared to a single decision tree? 
3. Why is XGBoost often preferred for structured/tabular datasets? 
4. What is stacking in ensemble learning, and how does it work? 
5. Can ensemble methods overfit? How do you prevent it? 
6. What are the advantages and drawbacks of using ensemble models? 
7. When would a simple model outperform an ensemble? 
8. How does voting work in classification ensembles? 
9. What are feature importances in ensemble models, and how are they useful? 
10. Why might ensemble models be harder to interpret?

[Linkedin](https://www.linkedin.com/feed/update/urn:li:activity:7318473321723936768/)
