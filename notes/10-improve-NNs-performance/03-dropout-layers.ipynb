{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions to Overfitting\n",
    "1. Add more data\n",
    "2. Reduce Complexity\n",
    "3. Early Stopping\n",
    "4. Regularization\n",
    "5. Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Dropout Layer \n",
    "Imagine you're teaching a classroom of students (your neural network) to solve problems. Normally, you'd have all students working together every time. But with dropout, you randomly send some students out of the room during each lesson.\n",
    "\n",
    "Here's why this works:\n",
    "- It prevents the class from relying too much on any one \"star student\"\n",
    "- Forces the remaining students to pick up the slack\n",
    "- Makes the whole group more adaptable\n",
    "\n",
    "## How It Actually Works in Code\n",
    "\n",
    "When you add dropout to a neural network layer, here's what happens:\n",
    "\n",
    "```python\n",
    "# This is how you'd write it in Keras\n",
    "model.add(Dense(128, activation='relu'))  # A normal layer\n",
    "model.add(Dropout(0.4))  # This randomly turns off 40% of neurons during training\n",
    "```\n",
    "\n",
    "During training:\n",
    "- At each step, 40% of the neurons in that layer get temporarily turned off\n",
    "- Different neurons get dropped each time\n",
    "- During testing, all neurons stay on (but their outputs get scaled down by 40%)\n",
    "\n",
    "## Why This Is So Useful\n",
    "\n",
    "1. **Stops Overfitting**: Like preventing students from memorizing answers\n",
    "2. **Makes the Network Tougher**: Forces it to develop multiple ways to solve problems\n",
    "3. **Works With Everything**: Plays nice with other techniques like BatchNorm\n",
    "\n",
    "## Practical Tips From Experience\n",
    "\n",
    "- **Where to Use It**: After dense or convolutional layers\n",
    "- **How Much to Drop**: Start with 0.2 (20%) and adjust\n",
    "  - More complex networks can handle higher dropout (up to 0.5)\n",
    "  - Simple networks might need less (0.1-0.3)\n",
    "- **Watch for Signs**:\n",
    "  - If your network isn't learning enough (underfitting), reduce dropout\n",
    "  - If it's memorizing the training data (overfitting), increase dropout\n",
    "\n",
    "## Common Mistakes to Avoid\n",
    "\n",
    "1. Using dropout on every single layer (often too much)\n",
    "2. `Forgetting that dropout is only active during training and at the testing time all nodes are active based on (1-p) probability`\n",
    "3. Using very high dropout rates (>0.5) unless you have a good reason\n",
    "\n",
    "Dropout is like giving your network \"tough love\" training - it might complain at first, but it'll perform better in the real world because of it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okela\n"
     ]
    }
   ],
   "source": [
    "print(\"okela\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (pywrap_tensorflow_internal.py, line 114)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[36m(most recent call last)\u001b[39m:\n",
      "  File \u001b[92md:\\Ao\\Code\\AI\\deep-learning\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3667\u001b[39m in \u001b[95mrun_code\u001b[39m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  Cell \u001b[92mIn[5]\u001b[39m\u001b[92m, line 1\u001b[39m\n    import tensorflow\n",
      "  File \u001b[92md:\\Ao\\Code\\AI\\deep-learning\\venv\\Lib\\site-packages\\tensorflow\\__init__.py:24\u001b[39m\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\n",
      "  File \u001b[92md:\\Ao\\Code\\AI\\deep-learning\\venv\\Lib\\site-packages\\tensorflow\\python\\__init__.py:49\u001b[39m\n    from tensorflow.python import pywrap_tensorflow\n",
      "\u001b[36m  \u001b[39m\u001b[36mFile \u001b[39m\u001b[32md:\\Ao\\Code\\AI\\deep-learning\\venv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:58\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfrom tensorflow.python.pywrap_tensorflow_internal import *\u001b[39m\n",
      "  \u001b[36mFile \u001b[39m\u001b[32md:\\Ao\\Code\\AI\\deep-learning\\venv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py:114\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef TFE_ContextOptionsSetAsync(arg1, async):\u001b[39m\n                                         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "from tensorflow.python.pywrap_tensorflow_internal import *\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
