# Deep and Bidirectional RNN Architectures

## Deep RNNs (Stacked RNNs)

### What are Deep RNNs?
Deep RNNs stack multiple recurrent layers on top of each other to create hierarchical representations of sequential data. Each layer processes the sequence and passes its understanding to the next layer.

**Key Characteristics:**
- Multiple hidden RNN layers
- Higher layers capture more abstract patterns
- Increased model capacity compared to single-layer RNNs

### Implementation Examples:

```python
# Stacked SimpleRNN
model.add(SimpleRNN(64, return_sequences=True))  # First layer
model.add(SimpleRNN(64))                         # Second layer

# Stacked LSTM
model.add(LSTM(128, return_sequences=True))     # First LSTM layer
model.add(LSTM(128))                            # Second LSTM layer

# Stacked GRU
model.add(GRU(64, return_sequences=True))       # First GRU layer
model.add(GRU(64))                              # Second GRU layer
```

**When to Use:**
- For complex sequence patterns
- When you have sufficient training data
- When single-layer RNNs underfit your data

## Bidirectional RNNs

### Concept:
Bidirectional RNNs process sequences in both forward and backward directions, combining information from past and future context.

**Key Advantages:**
- Captures context from both directions
- Often outperforms unidirectional RNNs
- Particularly effective for NLP tasks

### Variants:

1. **Bidirectional SimpleRNN**
   ```python
   model.add(Bidirectional(SimpleRNN(64)))
   ```

2. **Bidirectional LSTM (BiLSTM)**
   ```python
   model.add(Bidirectional(LSTM(128)))
   ```

3. **Bidirectional GRU (BiGRU)**
   ```python
   model.add(Bidirectional(GRU(64)))
   ```

### Architecture Details:
```
Forward Layer:  [word1] → [word2] → [word3] → [word4]
Backward Layer: [word1] ← [word2] ← [word3] ← [word4]
Combined Output: Concatenation of forward and backward states
```

## Practical Considerations

### When to Use Deep vs. Bidirectional:

| Architecture | Best For | Computation Cost |
|--------------|----------|------------------|
| Stacked RNNs | Hierarchical patterns | Medium |
| Stacked LSTMs | Long-range dependencies | High |
| Stacked GRUs | Similar to LSTM but faster | Medium-High |
| Bidirectional | Context-dependent tasks | 2x unidirectional |

### Performance Tips:
1. Start with 2-3 layers for stacked architectures
2. Use dropout between RNN layers to prevent overfitting
3. For bidirectional RNNs, reduce hidden units to control parameters
4. Monitor training time - deeper/bidirectional models train slower

## Real-World Applications

**Stacked Architectures:**
- Machine translation (deep encoder-decoder)
- Video analysis (temporal hierarchies)
- Complex time-series forecasting

**Bidirectional:**
- Named entity recognition
- Speech recognition
- Sentiment analysis
- Question answering systems

Example: A 3-layer BiLSTM achieved state-of-the-art results on many NLP benchmarks before the Transformer era.